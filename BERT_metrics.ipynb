{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook for running transformer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration_electra import ElectraConfig\n",
    "from modeling_electra import ElectraModel\n",
    "from modeling_electra import ElectraLayer\n",
    "from transformers import ElectraTokenizerFast\n",
    "\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe5049321d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "with open(\"data/BERT_benchmark_train_ablation.json\", 'r') as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openwebtext (/home/dongpeijie/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cb8a3472b7437c986312e9508b4448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dongpeijie/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521/cache-8daa6c5ff9af9a7e_*_of_00032.arrow\n"
     ]
    }
   ],
   "source": [
    "# Target dataset is openwebtex\n",
    "dataset = load_dataset(\"openwebtext\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_dataset = dataset.map(encode, batched=True, num_proc=32)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample tokenized batch from dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=128)\n",
    "inputs = tokenizer(next(iter(dataloader))['text'], truncation=True, padding='max_length', return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance calculations for Jacobian covariance and variations\n",
    "def covariance(jacobs):\n",
    "    jacob = torch.transpose(jacobs, 0, 1).reshape(jacobs.size(1), -1).cpu().numpy()\n",
    "    correlations = np.corrcoef(jacob)\n",
    "    v, _ = np.linalg.eig(correlations)\n",
    "    k = 1e-5\n",
    "    return -np.sum(np.log(v + k) + 1.0 / (v + k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine calculations for Jacobian cosine and variations\n",
    "def cosine(jacobs):\n",
    "    jacob = torch.transpose(jacobs, 0, 1).reshape(jacobs.size(1), -1).cpu().numpy()\n",
    "    norm = np.linalg.norm(jacob, axis=1)\n",
    "    normed = jacob / norm[:, None]\n",
    "    cosines = (-pairwise_distances(normed, metric=\"cosine\") + 1) - np.identity(\n",
    "        normed.shape[0]\n",
    "    )\n",
    "    summed = np.sum(np.power(np.absolute(cosines.flatten()), 1.0 / 20)) / 2\n",
    "    return 1 - (1 / (pow(cosines.shape[0], 2) - cosines.shape[0]) * summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synaptic Diversity metric\n",
    "def synaptic_diversity(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ):\n",
    "                        metric_array.append(\n",
    "                            torch.abs(\n",
    "                                torch.norm(sublayer.weight, \"nuc\")\n",
    "                                * torch.norm(sublayer.weight.grad, \"nuc\")\n",
    "                            )\n",
    "                        )\n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synaptic_diversity_normalized(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ):\n",
    "                        metric_array.append(\n",
    "                            torch.abs(\n",
    "                                torch.norm(sublayer.weight, \"nuc\")\n",
    "                                * torch.norm(sublayer.weight.grad, \"nuc\")\n",
    "                            )\n",
    "                        )\n",
    "    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synaptic saliency metric\n",
    "def synaptic_saliency(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.intermediate.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "            for sublayer in layer.output.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "                    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synaptic_saliency_normalized(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.intermediate.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "            for sublayer in layer.output.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "                    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Distance metric\n",
    "def activation_distance(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        output = output[0].view(output.size(1), -1)\n",
    "        x = (output > 0).float()\n",
    "        K = x @ x.t()\n",
    "        K2 = (1.0 - x) @ (1.0 - x.t())\n",
    "        metric_array.append(K + K2)\n",
    "        \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(outputs)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_distance_normalized(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        output = output[0].view(output.size(1), -1)\n",
    "        x = (output > 0).float()\n",
    "        K = x @ x.t()\n",
    "        K2 = (1.0 - x) @ (1.0 - x.t())\n",
    "        metric_array.append(K + K2)\n",
    "        \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(outputs)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_score(model):\n",
    "    jacobs = model.embeddings.position_embeddings.weight.grad.detach()\n",
    "    return covariance(jacobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_score_cosine(model):\n",
    "    jacobs = model.embeddings.position_embeddings.weight.grad.detach()\n",
    "    return cosine(jacobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Head Importance metric\n",
    "def head_importance(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ) and sublayer.weight.shape[0] >= 128:\n",
    "                        metric_array.append(\n",
    "                            torch.abs(sublayer.weight.data * sublayer.weight.grad)\n",
    "                        )\n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_importance_normalized(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ) and sublayer.weight.shape[0] >= 128:\n",
    "                        metric_array.append(\n",
    "                            torch.abs(sublayer.weight.data * sublayer.weight.grad)\n",
    "                        )\n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Confidence metric (for both head and softmax)\n",
    "def attention_condfidence(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        metric_array.append(torch.mean(torch.max(output, 1)[0]))\n",
    "    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(outputs)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_condfidence_normalized(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        metric_array.append(torch.mean(torch.max(output, 1)[0]))\n",
    "    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongpeijie/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">84</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output.backward(torch.ones_like(output))                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 82 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 83 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>row = [configs[i][<span style=\"color: #808000; text-decoration-color: #808000\">\"id\"</span>],                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 84 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │      </span>configs[i][<span style=\"color: #808000; text-decoration-color: #808000\">\"scores\"</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"glue\"</span>],                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 85 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │      </span>synaptic_diversity(model),                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 86 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │      </span>synaptic_diversity_normalized(model),                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │      </span>synaptic_saliency(model),                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'glue'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m84\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   \u001b[0moutput.backward(torch.ones_like(output))                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 82 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   \u001b[0mrow = [configs[i][\u001b[33m\"\u001b[0m\u001b[33mid\u001b[0m\u001b[33m\"\u001b[0m],                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 84 \u001b[2m│   │   │      \u001b[0mconfigs[i][\u001b[33m\"\u001b[0m\u001b[33mscores\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mglue\u001b[0m\u001b[33m\"\u001b[0m],                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   │      \u001b[0msynaptic_diversity(model),                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 86 \u001b[0m\u001b[2m│   │   │      \u001b[0msynaptic_diversity_normalized(model),                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   │   │      \u001b[0msynaptic_saliency(model),                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'glue'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run metrics on all model in benchmark\n",
    "with open(\"BERT_initialization_ablation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"ID\",\n",
    "              \"GLUE Score\",\n",
    "              \"Synaptic Diversity\",\n",
    "              \"Synaptic Diversity Normalized\",\n",
    "              \"Synaptic Saliency\",\n",
    "              \"Synaptic Saliency Normalized\",\n",
    "              \"Activation Distance\",\n",
    "              \"Activation Distance Normalized\",\n",
    "              \"Jacobian Score\",\n",
    "              \"Jacobian Score Normalized\",\n",
    "              \"Number of Parameters\",\n",
    "              \"Head Importance\",\n",
    "              \"Head Importance Normalized\",\n",
    "              \"Head Confidence\",\n",
    "              \"Head Confidence Normalized\",\n",
    "              \"Head Softmax Confidence\",\n",
    "              \"Head Softmax Confidence Normalized\",\n",
    "             ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for i in range(500):\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        nas_config = configs[i][\"hparams\"][\"model_hparam_overrides\"][\"nas_config\"]\n",
    "\n",
    "        config = ElectraConfig(\n",
    "            nas_config=nas_config, num_hidden_layers=len(nas_config[\"encoder_layers\"]), output_hidden_states=True\n",
    "        )\n",
    "        model = ElectraModel(config)\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        \n",
    "        # Hooks to get outputs at different layers\n",
    "        activation_outputs = []\n",
    "        def activation_hook(module, input, output):\n",
    "            activation_outputs.append(output)\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, ElectraLayer):\n",
    "                sublayer = layer.intermediate.intermediate_act_fn.register_forward_hook(activation_hook)\n",
    "\n",
    "        head_outputs = []\n",
    "        def head_hook(module, input, output):\n",
    "            head_outputs.append(output)\n",
    "\n",
    "        # Initialize hooks\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, ElectraLayer):\n",
    "                sublayer = layer.operation.operation\n",
    "                if hasattr(sublayer, 'query'):\n",
    "                    sublayer.query.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'key'):\n",
    "                    sublayer.key.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'value'):\n",
    "                    sublayer.value.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'input'):\n",
    "                    sublayer.input.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'weight'):\n",
    "                    sublayer.weight.register_forward_hook(head_hook)\n",
    "\n",
    "        softmax_outputs = []\n",
    "        def softmax_hook(module, input, output):\n",
    "            softmax_outputs.append(output)\n",
    "\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, ElectraLayer):\n",
    "                sublayer = layer.operation.operation\n",
    "                if hasattr(sublayer, 'softmax'):\n",
    "                    sublayer.softmax.register_forward_hook(softmax_hook)\n",
    "\n",
    "        # Run gradient with respect to ones\n",
    "        model.zero_grad()\n",
    "        output = model(**inputs).last_hidden_state\n",
    "        output.backward(torch.ones_like(output))\n",
    "\n",
    "        row = [configs[i][\"id\"],\n",
    "               configs[i][\"scores\"][\"glue\"],\n",
    "               synaptic_diversity(model),\n",
    "               synaptic_diversity_normalized(model),\n",
    "               synaptic_saliency(model),\n",
    "               synaptic_saliency_normalized(model),\n",
    "               activation_distance(activation_outputs),\n",
    "               activation_distance_normalized(activation_outputs),\n",
    "               jacobian_score(model),\n",
    "               jacobian_score_cosine(model),\n",
    "               num_parameters(model),\n",
    "               head_importance(model),\n",
    "               head_importance_normalized(model),\n",
    "               attention_condfidence(head_outputs),\n",
    "               attention_condfidence_normalized(head_outputs),\n",
    "               attention_condfidence(softmax_outputs),\n",
    "               attention_condfidence_normalized(softmax_outputs),\n",
    "              ]\n",
    "        \n",
    "        writer.writerow(row)\n",
    "        f.flush()\n",
    "\n",
    "        print(str(configs[i][\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_models = [285,\n",
    "116,\n",
    "280,\n",
    "337,\n",
    "464,\n",
    "166,\n",
    "153,\n",
    "157,\n",
    "330,\n",
    "164,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>np.random.seed(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.manual_seed(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 33 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>nas_config = configs[i][<span style=\"color: #808000; text-decoration-color: #808000\">\"hparams\"</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"model_hparam_overrides\"</span>][<span style=\"color: #808000; text-decoration-color: #808000\">\"nas_config\"</span>]         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>config = ElectraConfig(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>nas_config=nas_config, num_hidden_layers=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(nas_config[<span style=\"color: #808000; text-decoration-color: #808000\">\"encoder_layers\"</span>]),    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>list index out of range\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m33\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   │   \u001b[0mnp.random.seed(\u001b[94m0\u001b[0m)                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0m\u001b[2m│   │   \u001b[0mtorch.manual_seed(\u001b[94m0\u001b[0m)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 33 \u001b[2m│   │   \u001b[0mnas_config = configs[i][\u001b[33m\"\u001b[0m\u001b[33mhparams\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mmodel_hparam_overrides\u001b[0m\u001b[33m\"\u001b[0m][\u001b[33m\"\u001b[0m\u001b[33mnas_config\u001b[0m\u001b[33m\"\u001b[0m]         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 35 \u001b[0m\u001b[2m│   │   \u001b[0mconfig = ElectraConfig(                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m\u001b[2m│   │   │   \u001b[0mnas_config=nas_config, num_hidden_layers=\u001b[96mlen\u001b[0m(nas_config[\u001b[33m\"\u001b[0m\u001b[33mencoder_layers\u001b[0m\u001b[33m\"\u001b[0m]),    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mlist index out of range\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run metrics on ablation study of different initialization states and minibatch inputs\n",
    "with open(\"BERT_batch_ablation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"ID\",\n",
    "              \"GLUE Score\",\n",
    "              \"Synaptic Diversity\",\n",
    "              \"Synaptic Diversity Normalized\",\n",
    "              \"Synaptic Saliency\",\n",
    "              \"Synaptic Saliency Normalized\",\n",
    "              \"Activation Distance\",\n",
    "              \"Activation Distance Normalized\",\n",
    "              \"Jacobian Score\",\n",
    "              \"Jacobian Score Normalized\",\n",
    "              \"Number of Parameters\",\n",
    "              \"Head Importance\",\n",
    "              \"Head Importance Normalized\",\n",
    "              \"Head Confidence\",\n",
    "              \"Head Confidence Normalized\",\n",
    "              \"Head Softmax Confidence\",\n",
    "              \"Head Softmax Confidence Normalized\",\n",
    "             ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for i in ablation_models:\n",
    "        # comment to investigate initialization ablation\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "        \n",
    "        nas_config = configs[i][\"hparams\"][\"model_hparam_overrides\"][\"nas_config\"]\n",
    "\n",
    "        config = ElectraConfig(\n",
    "            nas_config=nas_config, num_hidden_layers=len(nas_config[\"encoder_layers\"]), output_hidden_states=True\n",
    "        )\n",
    "        model = ElectraModel(config)\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        \n",
    "        # uncomment to investigate minibatch ablation\n",
    "        # iterator = iter(dataloader)\n",
    "        \n",
    "        for j in range(10):\n",
    "            # tokenizer(next(iterator)['text'], truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "            # inputs.to(device)\n",
    "\n",
    "            np.random.seed(j)\n",
    "            torch.manual_seed(j)\n",
    "\n",
    "            activation_outputs = []\n",
    "            def activation_hook(module, input, output):\n",
    "                activation_outputs.append(output)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.intermediate.intermediate_act_fn.register_forward_hook(activation_hook)\n",
    "\n",
    "            head_outputs = []\n",
    "            def head_hook(module, input, output):\n",
    "                head_outputs.append(output)\n",
    "\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.operation.operation\n",
    "                    if hasattr(sublayer, 'query'):\n",
    "                        sublayer.query.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'key'):\n",
    "                        sublayer.key.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'value'):\n",
    "                        sublayer.value.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'input'):\n",
    "                        sublayer.input.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'weight'):\n",
    "                        sublayer.weight.register_forward_hook(head_hook)\n",
    "\n",
    "            softmax_outputs = []\n",
    "            def softmax_hook(module, input, output):\n",
    "                softmax_outputs.append(output)\n",
    "\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.operation.operation\n",
    "                    if hasattr(sublayer, 'softmax'):\n",
    "                        sublayer.softmax.register_forward_hook(softmax_hook)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "            output = model(**inputs).last_hidden_state\n",
    "            output.backward(torch.ones_like(output))\n",
    "\n",
    "            row = [configs[i][\"id\"],\n",
    "                   configs[i][\"scores\"][\"glue\"],\n",
    "                   synaptic_diversity(model),\n",
    "                   synaptic_diversity_normalized(model),\n",
    "                   synaptic_saliency(model),\n",
    "                   synaptic_saliency_normalized(model),\n",
    "                   activation_distance(activation_outputs),\n",
    "                   activation_distance_normalized(activation_outputs),\n",
    "                   jacobian_score(model),\n",
    "                   jacobian_score_cosine(model),\n",
    "                   num_parameters(model),\n",
    "                   head_importance(model),\n",
    "                   head_importance_normalized(model),\n",
    "                   attention_condfidence(head_outputs),\n",
    "                   attention_condfidence_normalized(head_outputs),\n",
    "                   attention_condfidence(softmax_outputs),\n",
    "                   attention_condfidence_normalized(softmax_outputs),\n",
    "                  ]\n",
    "\n",
    "            writer.writerow(row)\n",
    "            f.flush()\n",
    "\n",
    "            print(str(configs[i][\"id\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
